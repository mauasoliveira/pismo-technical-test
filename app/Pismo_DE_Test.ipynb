{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fbf278b-168f-4602-8e50-69988739249c",
   "metadata": {},
   "source": [
    "# Pismo Data Engineering Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb62d768-cc19-40f7-b91c-960a5aa8c1df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Architecture\n",
    "\n",
    "This architecture was created to demonstrate a cluster and to simulate a cloud provider. \n",
    "\n",
    "A initial standalone application was made, but after thought and incentives, this was enriched to this format. \n",
    "\n",
    "The test is entirely implemented on this Notebook and the whole application is suited on docker structure.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "* Docker environment\n",
    "* Ports 4040, 8080, 8088, 8888 and 9870 available \n",
    "\n",
    "## Docker Stack\n",
    "\n",
    "* Hadoop Cluster, with:\n",
    "  - [namenode](http://localhost:9870)\n",
    "  - [resource manager](http://localhost:8088)\n",
    "  - one datanode\n",
    "  - one node manager\n",
    "* Spark Cluster, with\n",
    "  - [Spark Master](http://localhost:8080)\n",
    "  - Two Workers\n",
    "* Jupyter Notebook, with\n",
    "  - [Spark Job status](http://localhost:4040) - as Spark does not support full cluster mode with Python, the status application needs to run on the caller\n",
    "  - [Jupyter Environment](http://localhost:8888) - This notebook\n",
    "\n",
    "## Images\n",
    "\n",
    "The project runs on official images of Apache, Jupyter and Bitnami.\n",
    "\n",
    "Bitnami Spark image was chosen due to the easy of use and pre-configuration to clustering.\n",
    "\n",
    "## Considerations\n",
    "\n",
    "* I implemented with a local Hadoop cluster to make a full demonstration of Spark and the solution. On production environment, this will probably be hosted on some cloud platform (S3, GCS, OCS etc)\n",
    "* The Jupyter Notebook is just to facilitate the usage and documentation. All scripts can be executed as DAGs or applications.\n",
    "* This solution can easily be transposed to a Helm structure, to be executed on Kubernetes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66932421-973d-465c-b4a0-bdcbeedca499",
   "metadata": {},
   "source": [
    "# Execution steps\n",
    "\n",
    "## Generate Data\n",
    "\n",
    "The data is not persisted on host, being generated based on the script taken from [Eder's Github](https://github.com/eder1985/pismo_recruiting_technical_case/tree/main) with a few changes: \n",
    "\n",
    "* Added execution batches\n",
    "* Added total registries to create in each batch\n",
    "* Added duplication rate\n",
    "* Saving on HDFS - based on this architecture\n",
    "* Unique filenames\n",
    "\n",
    "The next cell will generate the fake data. To check the progress, visit [Hadoop Namenode](http://localhost:9870/explorer.html#/pismo-data/source) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1581dc70-819e-4fe9-b40a-f1b34f810f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "\n",
    "from local_fake_data_generator import main as generate_fake_data\n",
    "\n",
    "# Configure here the parameters to generate the data\n",
    "batches = 5\n",
    "total_per_file = 10_000\n",
    "duplication_rate = 0.1\n",
    "\n",
    "generate_fake_data(batches, total_per_file, duplication_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c911569b-629b-40f2-ad10-3f2772e3c47b",
   "metadata": {},
   "source": [
    "## Start processing\n",
    "\n",
    "The next cell is responsible for creating the connection to [Apache Spark](http://localhost:8080). \n",
    "\n",
    "Job status can be monitored from [Job Page](http://localhost:4040)\n",
    "\n",
    "Basic data validation is done on source: \n",
    "* Assert is not empty\n",
    "* Assert the required columns are present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e939bee1-d9c0-48af-93ac-1e7eff117db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from os import getenv\n",
    "\n",
    "# Configurations\n",
    "BASE_DATA = getenv('HDFS_NAMENODE') + '/pismo-data/'\n",
    "SOURCE_DATA = BASE_DATA + 'source/'\n",
    "OUTPUT_DATA = BASE_DATA + 'output/'\n",
    "\n",
    "## Initialize Spark Session\n",
    "spark = SparkSession.builder.master(getenv('SPARK_MASTER')).getOrCreate()\n",
    "\n",
    "# Read data from /pismo-data/\n",
    "source = spark.read.option('multiline', True).json( SOURCE_DATA + '*.json')\n",
    "\n",
    "# Basic data validations over datasource\n",
    "assert isinstance(source,  DataFrame), type(source)\n",
    "assert not source.isEmpty()\n",
    "for column in [ 'domain', 'event_id', 'event_type', 'timestamp', 'data']: \n",
    "    assert column in source.columns\n",
    "\n",
    "source.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228387a1-50e0-476a-ba15-6d33c15363b5",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "Here the data will be processed. \n",
    "\n",
    "To implement the rules, I opted to not use UDFs as they are very slow in processing time. All processing is done via native functions.\n",
    "\n",
    "1. Add columns `unique_event` (based on `domain`+`event_type` and `domain_id` (`data.id` to the sources\n",
    "2. De-duplicate data:\n",
    "   * Create a windowed partition over `unique_event` and `domain_id`\n",
    "   * Order by the most recent first\n",
    "   * Filter by `row_number == 1`\n",
    "3. Add columns based on data timestamp - to be able to partition afterwards\n",
    "\n",
    "At the end, the cell shows some information about how many duplicated registries were removed (based on `domain`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6115c9e-6e72-4bb5-be24-6a1953c29007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Adding identifiers column\n",
    "print('Adding unique event and domain ids')\n",
    "domain_data = source\\\n",
    "    .withColumn('unique_event', F.concat(col('domain'), col('event_type')))\\\n",
    "    .withColumn('domain_id', col('data.id'))\n",
    "\n",
    "# Dropping Duplicates over unique_event and domain_id\n",
    "print('Deduplicating data...')\n",
    "pismo_window = Window.partitionBy(['unique_event', 'domain_id']).orderBy(col('timestamp').desc())\n",
    "windowed_data =  domain_data\\\n",
    "    .withColumn('row_number', F.row_number().over(pismo_window))\n",
    "\n",
    "duplicated = windowed_data.where(col('row_number') > 1)\n",
    "\n",
    "pismo_data = windowed_data\\\n",
    "    .where(col('row_number') == 1)\\\n",
    "    .drop('row_number')\n",
    "\n",
    "## Time partitioning\n",
    "print('Time partitioning... ')\n",
    "date_column = 'timestamp'\n",
    "final = pismo_data\\\n",
    "    .withColumn('year', F.year(date_column))\\\n",
    "    .withColumn('month', F.month(date_column))\\\n",
    "    .withColumn('day', F.dayofmonth(date_column))\\\n",
    "    .drop('unique_event')\\\n",
    "    .drop('domain_id')\n",
    "\n",
    "print('Duplicated data: ')\n",
    "duplicated.groupBy(col('domain')).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525dd10-ab65-46bc-a601-80f89581deaf",
   "metadata": {},
   "source": [
    "## Saving\n",
    "\n",
    "This step saves the output on HDFS cluster with Parquet format, partitioned by `domain`+`date` (broken down). Example:\n",
    "```\n",
    "domain=transaction/\n",
    "  year=2024/\n",
    "    month=9/\n",
    "        day=10/\n",
    "            file0001.parquet\n",
    "            file0002.parquet\n",
    "```\n",
    "\n",
    "After processing, the full output can be seen on [HDFS Namenode](http://localhost:9870/explorer.html#/pismo-data/output/)\n",
    "\n",
    "### TODO\n",
    "\n",
    "* The only thing that I couldn't do here is to put a progress bar or similar to be able to monitor Spark Application. For now, it can be done on [Status Job](http://localhost:4040)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbba653-3107-4007-a19f-c09e1fb77190",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Saving to {OUTPUT_DATA}')\n",
    "final\\\n",
    "    .repartition('domain', 'timestamp')\\\n",
    "    .write.mode('overwrite')\\\n",
    "    .partitionBy(['domain', 'year', 'month', 'day'])\\\n",
    "    .parquet(OUTPUT_DATA)\n",
    "\n",
    "print('Saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb8952-8951-46a2-9368-b659187aa879",
   "metadata": {},
   "source": [
    "## Validation over saved data \n",
    "\n",
    "This step reads the recently processed data and validates the output.\n",
    "\n",
    "Besides the above basic validation, it checks if data was truly de-duplicated (counts only)\n",
    "\n",
    "Note: the `recursiveFileLookup` allows Spark to read based on the partition. This is not used here, but is very useful when filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b84814-f79b-4474-ade0-2a2cdcf43320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation over Parquet data\n",
    "\n",
    "validation = spark.read.option('recursiveFileLookup', False).parquet(OUTPUT_DATA)\n",
    "\n",
    "# Basic data validations over datasource\n",
    "assert isinstance(validation,  DataFrame), type(source)\n",
    "assert not validation.isEmpty()\n",
    "for column in [ 'domain', 'event_id', 'event_type', 'timestamp', 'data']: \n",
    "    assert column in validation.columns\n",
    "\n",
    "source_count = source.count()\n",
    "validation_count = validation.count()\n",
    "\n",
    "assert source_count > validation_count, 'Expected to have duplicated data!'\n",
    "assert source_count - validation_count == duplicated.count(), 'Unexpected counts over duplicated data!'\n",
    "\n",
    "validation.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f48e4ac-fee6-4544-bd0f-f9b543816066",
   "metadata": {},
   "source": [
    "### Validation over transations\n",
    "\n",
    "To validate data over transactions, we read data and check if the schema checks with expected data: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ccc7d8-6c68-44a2-bd08-bcefb0d7d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_transactions = spark.read.option('recursiveFileLookup', False).parquet(OUTPUT_DATA).where(col('domain') == 'transaction').select('domain', 'event_id', 'event_type', 'timestamp', 'data.*')\n",
    "\n",
    "assert isinstance(valid_transactions, DataFrame)\n",
    "assert not valid_transactions.isEmpty()\n",
    "\n",
    "for _col in [ 'id', 'account_orig_id', 'account_dest_id', 'amount', 'currency' ]:\n",
    "    assert _col in valid_transactions.columns\n",
    "\n",
    "valid_transactions.groupBy('domain').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b6a04d-816e-4f3c-9fc4-7338932e714b",
   "metadata": {},
   "source": [
    "# Data is ready to be consumed!\n",
    "\n",
    "Below is a sample of getting the total of transactions by currency. \n",
    "\n",
    "> (based on fake data that was generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c1b24-5066-47df-a037-9178e5ba48c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = [ col(_col) for _col in validation.columns if _col != 'data' ]\n",
    "df_columns.append('data.*')\n",
    "\n",
    "transactions = validation.where(col('domain') == 'transaction').select(*df_columns) \n",
    "\n",
    "# Total `amount` by `new_status`\n",
    "transactions.groupBy('currency').sum('amount').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
